---
title: "Code Repository"
author: "Brendan Woods"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo    = FALSE,
  message = FALSE,
  warning = FALSE)

```


## Contents
- Pre-processing
- Creating the Composites using original EMERGE data
- Permutation Code
- Theoretical Distributions
- Latent Variable Analysis McMenamin et al.
- Power Estimation
- Type I error rate Estimation
- Simulating toy data set


## Pre-processing
```{r}
library(dplyr)
library(MASS)
library(tidyr)
library(mvtnorm)
library(ggplot2)
library(kableExtra)
library(stats)
library(nlme)
library(boot)
library(matrixcalc)
library(numDeriv)
library(cubature)
library(optimx)
library(brglm)
library(Matrix)
library(matrixcalc)

load("\\Users\\brend\\Downloads\\data")
tmpDat <- tmpDat[!(is.na(tmpDat$InsulinYN) & is.na(tmpDat$gw32) & is.na(tmpDat$gw38)), ] ## removes individuals with no observations for 3 outcomes
tmpDat$binary_insulin<-0
tmpDat$binary_insulin[tmpDat$InsulinYN == "Yes"]<-1
tmpDat$binary_group<-0
tmpDat$binary_group[tmpDat$Group == "Metformin"]<-1
tmpDat$binary_insulin<-as.numeric(tmpDat$binary_insulin)
tmpDat$binary_EMERGE<-0
tmpDat$binary_EMERGE[tmpDat$compositeYN == "Yes"]<-1
```


## Creating the Composites using original EMERGE data
```{r}
insulin_t_value<-t.test(tmpDat$binary_insulin[tmpDat$Group == "Metformin"], tmpDat$binary_insulin[tmpDat$Group == "Placebo"])$statistic
gw32_t_value<-t.test(tmpDat$gw32[tmpDat$Group == "Metformin"], tmpDat$gw32[tmpDat$Group == "Placebo"])$statistic
gw38_t_value<-t.test(tmpDat$gw38[tmpDat$Group == "Metformin"], tmpDat$gw38[tmpDat$Group == "Placebo"])$statistic

study_squared_composite<-(insulin_t_value^2)+(gw32_t_value^2)+(gw38_t_value^2)
study_summed_composite<-insulin_t_value+gw32_t_value+gw38_t_value

```


## Permutation Code
Permutation Function
```{r}
permutation_function<-function(x, y, z){

  permutation_summed_composite<-numeric(100000)
  permutation_squared_composite <- numeric(100000)
  i <-1

  repeat{

    tmpDat$Group<-sample(tmpDat$Group, nrow(tmpDat), replace = F)
    
    t1 <- t.test(x[tmpDat$Group == "Metformin"], x[tmpDat$Group == "Placebo"])$statistic
    t2 <- t.test(y[tmpDat$Group == "Metformin"], y[tmpDat$Group == "Placebo"])$statistic 
    t3 <- t.test(z[tmpDat$Group == "Metformin"], z[tmpDat$Group == "Placebo"])$statistic

    permutation_summed_composite[i] <- t1 + t2 +t3
    permutation_squared_composite[i]<-  t1^2 + t2^2 + t3^2
    
    i<- i +1

    if(i > 100000) break}

  list(permutation_summed_composite  = permutation_summed_composite,
    permutation_squared_composite = permutation_squared_composite)

}

```

Creating permuted null distributions using permutation function
```{r}
set.seed(24253739)
permuted_null_distributions <-permutation_function(tmpDat$gw32, tmpDat$gw38, tmpDat$binary_insulin)
permuted_null_distributions.df <-data.frame(permuted_null_distributions)
write.csv(permuted_null_distributions.df, "Permuted Null Distributions.csv", row.names = FALSE)
p_value_squared_composite <- sum(permuted_null_distributions.df$permutation_squared_composite >= study_squared_composite)/100000
p_value_summed_composite <- sum(permuted_null_distributions.df$permutation_summed_composite <= study_summed_composite)/100000
print(paste("P-value squared composite: ", p_value_squared_composite))
print(paste("P-value summed composite: ", p_value_summed_composite))
```

Plotting permuted null distributions
Summed test statistics composite
```{r warning=FALSE}
ggplot(permuted_null_distributions.df, aes(x= permutation_summed_composite))+
  geom_histogram(bins = 200, fill = "lightsalmon", color = "lightsalmon")+
  scale_x_continuous(limits = c(-10, 10)) +
  labs(x = "Test Statistic", y = "Frequency") +
  geom_vline(xintercept = quantile(permuted_null_distributions.df$permutation_summed_composite, 0.025), color = "red", linetype = "dashed", size = 0.75)+
    geom_vline(xintercept = quantile(permuted_null_distributions.df$permutation_summed_composite, 0.975), color = "red", linetype = "dashed", size = 0.75)+
  annotate("text", x = quantile(permuted_null_distributions.df$permutation_summed_composite, 0.025), y = 1250, label = "2.5th Percentile", color = "red", angle = 90, vjust = -0.5, size = 5)+
    annotate("text", x = quantile(permuted_null_distributions.df$permutation_summed_composite, 0.975), y = 1250, label = "97.5th Percentile", color = "red", angle = 90, vjust = -0.5, size = 5)+
  theme_minimal()+ theme(panel.grid.minor = element_blank()) +
  geom_vline( xintercept = study_summed_composite, color = "steelblue", size = 1 )+
  annotate("text", x = study_summed_composite, y = 1250, label = "Composite", color = "steelblue", angle = 90, vjust = -0.5, size = 5) + theme(axis.title = element_text(size = 16))
```

Permuted Squared test statistics composite
```{r warning=FALSE}
ggplot(permuted_null_distributions.df, aes(x= permutation_squared_composite))+
  geom_histogram(bins = 200, fill = "darksalmon", color = "darksalmon")+
  scale_x_continuous(limits = c(0, 30)) +
  labs(x = "Test Statistic", y = "Frequency") +
  geom_vline(xintercept = quantile(permuted_null_distributions.df$permutation_squared_composite, 0.95), color = "red", linetype = "dashed", size = 0.75)+
  annotate("text", x = quantile(permuted_null_distributions.df$permutation_squared_composite, 0.95), y = 2500, label = "95th Percentile", color = "red", angle = 90, vjust = -0.5)+
  theme_minimal()+ theme(panel.grid.minor = element_blank(),
                         axis.title = element_text(size = 16)) +
  geom_vline(xintercept = study_squared_composite, color = "steelblue", size = 1 ) +
  annotate("text", x = study_squared_composite, y = 2500, label = "Composite", color = "steelblue", angle = 90, vjust = -0.5)
```



## Theoretical Distributions
Summed test statistics  with equal weighting
```{r}
set.seed(24253739)
multivariate <- tmpDat %>% dplyr::select(gw32, gw38, binary_insulin)
sigma <- cor(multivariate, use = "complete.obs")
w <- c(1, 1, 1)
variance <-w%*%sigma%*%w
size <- 1000000
null_normal_distribution <- rnorm(size, mean = 0, sd = sqrt(variance))
p_value <- sum(null_normal_distribution <= study_summed_composite)/size
print(paste("P-value: ", p_value))

null_normal_distribution.df <- data.frame(null_normal_distribution)
write.csv(null_normal_distribution.df, "Theoretical Summed null distribution.csv", row.names = FALSE )


size<-100000
null_normal_distribution_small <- rnorm(size, mean = 0, sd = sqrt(variance))
write.csv(null_normal_distribution_small, "Theoretical Summed null distribution small.csv", row.names = F)
```
the p-value can be estimated using pnorm() for the asymptotic distribution of the composite (SC)
```{r}
p_value_pnorm <- 2*pnorm(study_summed_composite, mean =0, sd = sqrt(variance))
```


Plotting theoretical null distribution of summed test statistic composite
```{r}
ggplot(data = null_normal_distribution.df, aes(x =null_normal_distribution )) + geom_histogram(bins =250, colour = "darksalmon", fill = "darksalmon") + labs(x = "Summed Composite Test Statistic (SC)", y = "Frequency") +
  theme_minimal() +   geom_vline(xintercept = quantile(null_normal_distribution.df$null_normal_distribution, probs = c(0.025, 0.975)), color = "red", linetype = "dashed", linewidth = 0.75) + 
  annotate("text", x = quantile(null_normal_distribution.df$null_normal_distribution, 0.025), y = 12000, label = "2.5th Percentile", color = "red", angle = 90, vjust = -0.5, size = 5)+
  annotate("text", x = quantile(null_normal_distribution.df$null_normal_distribution, 0.975), y = 12000, label = "97.5th Percentile", color = "red", angle = 90, vjust = -0.5, size = 5)+
  geom_vline( xintercept = study_summed_composite, color = "steelblue", size = 1 ) + 
  annotate("text", x = study_summed_composite, y = 10000, label = "EMERGE (SC = -9.1)", color = "steelblue", angle = 90, vjust = -0.5)
```


Squared test statistics with eigenvalue weighting
```{r}
set.seed(24253739)
multivariate <- tmpDat %>% dplyr::select(gw32, gw38, binary_insulin)
multivariate_cm <- cor(multivariate, use = "complete.obs")
eigen_values <- eigen(multivariate_cm)$values
size <- 1000000
simulated_chi_sq_matrix <- matrix(rchisq(size * 3, df = 1), ncol = 3)
eigen_values_chi_sq<- simulated_chi_sq_matrix %*%eigen_values  
theoretical_null_squared <-data.frame(eigen_values_chi_sq)

p_value <-sum(theoretical_null_squared >= study_squared_composite)/size
print(paste("P-value: ", p_value))

theoretical_null_squared <- data.frame(chi_sq = eigen_values_chi_sq)
write.csv(theoretical_null_squared, "Theoretical Squared null distribution.csv", row.names = FALSE )

size <- 100000
simulated_chi_sq_matrix_small <- matrix(rchisq(size * 3, df = 1), ncol = 3)
eigen_values_chi_sq_small<- simulated_chi_sq_matrix_small %*%eigen_values  
write.csv(eigen_values_chi_sq_small, "Theoretical Squared null distribution small.csv", row.names = F)
```
Plotting theoretical null distribution of squared test statistic composite with eigenvalue weights
```{r}
ggplot(data = theoretical_null_squared, aes(x =chi_sq )) + geom_histogram(bins =250, colour = "lightsalmon", fill = "lightsalmon") + labs(x = "Squared Composite Test Statistic (QC)", y = "Frequency") +
  theme_minimal() + 
  geom_vline(xintercept = quantile(theoretical_null_squared$chi_sq, probs = c(0.95)), color = "red", linetype = "dashed", linewidth = 0.75) +
  annotate("text", x = quantile(theoretical_null_squared$chi_sq, 0.95), y = 25000, label = "95th Percentile", color = "red", angle = 90, vjust = -0.5, size = 5) +
  theme(panel.grid.minor = element_blank())+
  geom_vline( xintercept = study_squared_composite, color = "steelblue", size = 1 ) + 
  annotate("text", x = study_squared_composite, y = 25000, label = "Composite (QC = 29.2)", color = "steelblue", angle = 90, vjust = -0.5, size = 5)
```


## Latent Variable Analysis McMenamin et al.
The following code is from McMenamin et al. 2021. It has been modified to remove the need for a baseline variable for the continuous variables.
```{r}
#Likelihood function

f<-function(X,dat)
{
  #data
  #dat<-dat[!is.na(dat[,3]),]#change this to baseline measure when it is added? 
  
  #parameters
  alpha0 <- X[1]
    alpha1 <- X[2]
       beta0 <- X[3]
         beta1 <- X[4]
           psi0 <- X[5]
             psi1 <- X[6]
  
  #covariance parameters
  sig1 <- exp(X[7]) 
    sig2 <- exp(X[8]) 
     rho12 <- 2*inv.logit(X[9])-1
       rho13 <- 2*inv.logit(X[10])-1
          rho23 <- 2*inv.logit(X[11])-1
  
  
  #Known cutoffs
  tau0 <- -Inf
   tau1 <- 0
     tau2 <- +Inf
  
  print(X)
  
  #model means
  muz1<-alpha0+alpha1*dat[,2]
   muz2<-beta0+beta1*dat[,2]
     muz3<-psi0+psi1*dat[,2]
  
  #conditional means
  muz3cond <- muz3+((rho13-rho12*rho23)*(dat[,3]-muz1)/(sqrt(sig1)*(1-(rho12)^2)))+((rho23-rho13*rho12)*(dat[,4]-muz2)/(sqrt(sig2)*(1-(rho12)^2)))
  
  #conditional covariance
  sigcond <- 1-(((rho13)^2-2*rho12*rho13*rho23+(rho23)^2)/(1-(rho12)^2))
  
  #continuous bivariate covariance
  matbiv11 <- (sig1)^2
   matbiv12 <- rho12*sig1*sig2
    matbiv22 <- (sig2)^2
      Sigbiv   <- matrix(c(matbiv11, matbiv12, matbiv12, matbiv22), nrow=2, ncol=2)#check
        Sigbiv <- (Sigbiv*t(Sigbiv))^0.5
  
  #upperlimits
  mulim0<-matrix( tau1-muz3cond, ncol=1)
   mulim1<-matrix( tau2-muz3cond, ncol=1)
  
  #binprob
  pr30<-apply(mulim0,1,function(x){return(pmvnorm(lower=-Inf,upper=x,mean=0,sigma = sigcond))})
   pr31<-apply(mulim1,1,function(x){return(pmvnorm(lower=-Inf,upper=x,mean=0,sigma = sigcond))})
     prz12<-dmvnorm(cbind(dat[,3],dat[,4]), c(mean(muz1), mean(muz2)), Sigbiv)
  
  #Likelihood
  
  #components of likelihood,k=0,1 (binary)
  l0<-log(pr30)+log(prz12)#k=0
  l1<-log(pr31-pr30)+log(prz12)#k=1
  
  data0 <- cbind(dat[,5],l0)#0
   data1 <- cbind(dat[,5],l1)#1
  
  #0
  data0[data0[,1]==1,2]<-0
  
  #1
  data1[data1[,1]==0,2]<-0
  
  
  t0 <- sum(data0[,2])
    t1 <- sum(data1[,2])
  
  #-log(likelihood)
  Tfinal<-sum(t0)+sum(t1)
  
  return(-Tfinal)
}

lowerlim <- c(-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf,-Inf)
upperlim <- c(+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf,+Inf)


##Probability of response
integrand<-function(Zint,meantreat,meanuntreat,mle)
{
  
  sigmahat=matrix(nrow=3,ncol=3)
    sigmahat[1,1]=(exp(mle[7]))^2
     sigmahat[2,1]=(2*inv.logit(mle[9])-1)*(exp(mle[7]))*exp(mle[8])
       sigmahat[3,1]=(2*inv.logit(mle[10])-1)*(exp(mle[7]))
           sigmahat[1,2]=sigmahat[2,1]
             sigmahat[2,2]=(exp(mle[8]))^2
               sigmahat[3,2]=(2*inv.logit(mle[11])-1)*(exp(mle[8]))
                 sigmahat[1,3]=sigmahat[3,1]
                   sigmahat[2,3]=sigmahat[3,2]
                     sigmahat[3,3]=1
  
  xtreat<-cbind(-meantreat[,1]+Zint[1], -meantreat[,2]+Zint[2], -meantreat[,3]+Zint[3])
    xuntreat<-cbind(-meanuntreat[,1]+Zint[1],-meanuntreat[,2]+Zint[2],-meanuntreat[,3]+Zint[3])
  
  pdftreat=dmvnorm(xtreat, mean=c(0,0,0),sigma=sigmahat)
     pdfuntreat=dmvnorm(xuntreat, mean=c(0,0,0),sigma=sigmahat)
  
  return(c(mean(pdftreat),mean(pdfuntreat)))
}


probofsuccess<-function(mle,n,dat,eta)
{
  n=n
  
  meantreat=cbind(cbind(rep(1,n),rep(1,n))%*%c(mle[1:2]),cbind(rep(1,n),rep(1,n))%*%c(mle[3:4]), 
                  cbind(rep(1,n),rep(1,n))%*%mle[5:6])   
  meanuntreat=cbind(cbind(rep(1,n),rep(0,n))%*%c(mle[1:2]),cbind(rep(1,n),rep(0,n))%*%c(mle[3:4]), 
                    cbind(rep(1,n),rep(0,n))%*%mle[5:6])     
  
  #lower and upper bounds
  minmean1=min(c(meantreat[,1],meanuntreat[,1]))
  minmean2=min(c(meantreat[,2],meanuntreat[,2]))
  minmean3=min(c(meantreat[,3],meanuntreat[,3]))

  maxmean1=max(c(meantreat[,1],meanuntreat[,1]))
  maxmean2=max(c(meantreat[,2],meanuntreat[,2]))
  maxmean3=max(c(meantreat[,3],meanuntreat[,3]))
  
  lower=c(qnorm(1e-15,minmean1,exp(mle[7])),qnorm(1e-15,minmean2,exp(mle[8])),qnorm(1e-15,minmean3,1))
 
  upper=c(eta[1],eta[2],0)
  
  a=cuhre(f=integrand,nComp=2,lower=lower,upper=upper,flags=list(verbose=0,final=1,pseudo.random=0,mersenne.seed=NULL),
          meantreat=meantreat,meanuntreat=meanuntreat,mle=mle)
  #return(c(a$value[1],a$value[2]))
  #return(c(a$value[1]-a$value[2],a$value[1],a$value[2]))
  return(c((log(a$integral[1]/(1-a$integral[1]))-log(a$integral[2]/(1-a$integral[2]))),log(a$integral[1]/a$integral[2]),
           a$integral[1]-a$integral[2],a$integral[1],a$integral[2]))
  #return(log(a$value[1]/a$value[2]))
}

#Partial derivatives

partials<-function(mle,n,dat,eta)
{
  p=length(mle)
  fit1<-probofsuccess(mle,n,dat,eta)
  fitOR<-fit1[1]
  fitRR<-fit1[2]
  fitRD<-fit1[3]
  partials.augbinOR<-as.vector(rep(0,p))
  partials.augbinRR<-as.vector(rep(0,p))
  partials.augbinRD<-as.vector(rep(0,p))
  
  for(i in 1:p){
    valueupdate=mle
    valueupdate[i]=valueupdate[i]+0.000001
    
    updateprobOR=probofsuccess(valueupdate,n,dat,eta)[1]
    updateprobRR=probofsuccess(valueupdate,n,dat,eta)[2]
    updateprobRD=probofsuccess(valueupdate,n,dat,eta)[3]
    
    partials.augbinOR[i]=(updateprobOR-fitOR)/0.000001
    partials.augbinRR[i]=(updateprobRR-fitRR)/0.000001
    partials.augbinRD[i]=(updateprobRD-fitRD)/0.000001
    
  }
  
  return(c(partials.augbinOR,partials.augbinRR,partials.augbinRD,fit1))
}



result.latent<-as.list(NULL)
results<-as.list(NULL)

#Function for treatment effects and CIs for both methods

LatVarfunc<-function(dat,eta){
  n=dim(dat)[1]
  
  #Starting values
  lm1<-lm(dat[,3]~dat[,2])
  lm2<-lm(dat[,4]~dat[,2])
  lm3<-lm(dat[,5]~dat[,2])
  sig1est<-log(sqrt(var(dat[,3])))
  sig2est<-log(sqrt(var(dat[,4])))
  rho12est<-log(((cor(dat[,3],dat[,4])+1)/2)/(1-(cor(dat[,3],dat[,4])+1)/2))
  rho13est<-log(((cor(dat[,3],dat[,5])+1)/2)/(1-(cor(dat[,3],dat[,5])+1)/2))
  rho23est<-log(((cor(dat[,4],dat[,5])+1)/2)/(1-(cor(dat[,4],dat[,5])+1)/2))
  
  X<-c(lm1$coef[1],lm1$coef[2],lm2$coef[1],lm2$coef[2],lm3$coef[1],lm3$coef[2],sig1est,sig2est,rho12est, rho13est,rho23est)
  X<-as.vector(X)
  
  #Latent variable
  
  mlefit=optimx(X,f,dat=dat,lower=lowerlim,upper=upperlim,method="nlminb",control=list(rel.tol=1e-12))
  mle<-coef(mlefit[1,])
  hess<-attr(mlefit,"details")["nlminb",]$nhatend
  print(hess)
  mlecov=ginv(hess)
  mlecov<-nearPD(mlecov)$mat
  se<-sqrt(diag(mlecov))
  print(se)
  
  part<-partials(mle,n,dat,eta)
  print(part)
  meanOR<-part[34]
  
  partsOR <- part[1:11]
  varianceOR=t(partsOR)%*%mlecov%*%partsOR
  varianceOR=varianceOR[1,1]
  
  meanRR<-part[35]
  partsRR<-part[12:22]
  varianceRR=t(partsRR)%*%mlecov%*%partsRR
  varianceRR=varianceRR[1,1]
  
  meanRD<-part[36]
  partsRD<-part[23:33]
  varianceRD=t(partsRD)%*%mlecov%*%partsRD
  varianceRD=varianceRD[1,1]
  
  CIOR<-c(meanOR-1.96*sqrt(varianceOR),meanOR,meanOR+1.96*sqrt(varianceOR))
  CIRR<-c(meanRR-1.96*sqrt(varianceRR),meanRR,meanRR+1.96*sqrt(varianceRR))
  CIRD<-c(meanRD-1.96*sqrt(varianceRD),meanRD,meanRD+1.96*sqrt(varianceRD))
  
  probresplat<-c(part[37],part[38])
  result.latent<-c(CIOR,CIRR,CIRD,probresplat)
  
  
  epshat1 <- dat[,3]-(mle[1]+mle[2]*dat[,2])
  epshat2 <- dat[,4]-(mle[3]+mle[4]*dat[,2])
  epshat3 <- dat[,5]-(mle[5]+mle[6]*dat[,2])
  epshat <- as.matrix(cbind(epshat1,epshat2,epshat3))
  
  sig1hat <- exp(mle[7]) 
  sig2hat <- exp(mle[8]) 
  ##Correlation coefficients (rho12, rho13, rho23) must lie in the interval (-1, 1). However, optimization routines like optimx work best on    unconstrained parameters, ranging from -Inf to +Inf. To enable this, the code uses the logistic (sigmoid) transformation to map an           unconstrained value to the range (0,1), and then linearly transforms it to (−1,1).##
  rho12hat <- 2*inv.logit(mle[9])-1
  rho13hat <- 2*inv.logit(mle[10])-1
  rho23hat <- 2*inv.logit(mle[11])-1
  
  SigHat<-matrix(data=c(sig1hat^2,rho12hat*sig1hat*sig2hat,rho13hat*sig1hat,rho12hat*sig1hat*sig2hat,sig2hat^2,
           rho23hat*sig2hat,rho13hat*sig1hat,rho23hat*sig2hat,1),ncol=3)
  modres <- diag((epshat)%*%ginv(SigHat)%*%t(epshat))

  results<-c(result.latent,modres,mle,se)
  results1 <- c(result.latent, mle, se)
  return(results1)
}
```

The output of the latvarfunc is as follows:
1	logOR_LCI	Lower 95% CI for the log odds ratio
2	logOR	Point estimate for the log odds ratio
3	logOR_UCI	Upper 95% CI for the log odds ratio
4	logRR_LCI	Lower 95% CI for the log relative risk
5	logRR	Point estimate for the log relative risk
6	logRR_UCI	Upper 95% CI for the log relative risk
7	RD_LCI	Lower 95% CI for the risk difference
8	RD	Point estimate for the risk difference
9	RD_UCI	Upper 95% CI for the risk difference


Reprocessing tmpDat into the suitable format "inverted_data" for the LatVarfunc
Requires setting columns in a specific order and inverting the data
Column 1 is patient ID
Column 2 is treatment (binary 0,1)
Column 3 is gw32
Column 4 is gw38
Column 5 is Insulin (binary 0, 1)

Make sure to remove ## for the code to run
```{r}
data <- tmpDat[,c("Label", "binary_group", "gw32", "gw38", "binary_insulin")]
inverted_data <- data[complete.cases(data), ]
inverted_data$binary_insulin <- ifelse(inverted_data$binary_insulin == 1, 0, 1)
inverted_data$gw32 <- inverted_data$gw32 * -1
inverted_data$gw38 <- inverted_data$gw38 * -1

inverted_eta <- c(-5.1, -5.1) ## these are the "cut-off" points for the continuous variables
results_lat_var <- LatVarfunc(inverted_data, inverted_eta)
names(results_lat_var) <- c("logOR_LCI", "logOR", "logOR_UCI", "logRR_LCI", "logRR", "logRR_UCI", "RD_LCI", "RD", "RD_UCI",
  "Prob_Treat", "Prob_Control", "alpha0", "alpha1", "beta0", "beta1", "psi0", "psi1", "log_sig1", "log_sig2",
  "logit_rho12", "logit_rho13", "logit_rho23", "se_alpha0", "se_alpha1", "se_beta0", "se_beta1", "se_psi0", "se_psi1",
  "se_log_sig1", "se_log_sig2", "se_logit_rho12", "se_logit_rho13", "se_logit_rho23")
results_lat_var_df <- as.data.frame(t(results_lat_var))

log_odds_ratio    <- round(as.numeric(results_lat_var_df[1, 1:3]), 3)
log_risk_ratio    <- round(as.numeric(results_lat_var_df[1, 4:6]), 3)
risk_difference   <- round(as.numeric(results_lat_var_df[1, 7:9]), 3)

lat_var_results_table <- data.frame(Measure = c("Log Odds Ratio", "Log Risk Ratio", "Risk Difference"),
  "Lower Confidence Interval" = c(log_odds_ratio[1], log_risk_ratio[1], risk_difference[1]),
  `Point Estimate` = c(log_odds_ratio[2], log_risk_ratio[2], risk_difference[2]),
  `Upper Confidence Interval` = c(log_odds_ratio[3], log_risk_ratio[3], risk_difference[3])
)
write.csv(lat_var_results_table, "Latent variable results table.csv", row.names = FALSE)
```

Power estimation of Latent variable analysis
*Note that this takes a long time to run, and as such we only ran it for two sample sizes, 100 and 200*
Remove the double hashes to execute the power function
```{r}
set.seed(24253739)
lat_var_power_function<- function(n) {
  power_result <- matrix(NA, nrow = 100, ncol = 3) 
  i <- 1
  repeat {
    placebo_group <- inverted_data[inverted_data$binary_group == 0, ]
    sampled_placebo <- placebo_group[sample(nrow(placebo_group), n/2, replace = TRUE), ]
    
    metformin_group <- inverted_data[inverted_data$binary_group == 1, ]
    sampled_metformin <- metformin_group[sample(nrow(metformin_group), n/2, replace = TRUE), ]
    
    sampled_data <- rbind(sampled_placebo, sampled_metformin) ##Needed for latvarfunc?
    inverted_eta <- c(-5.1, -5.1)

    LatVarfunc_result1<- LatVarfunc(sampled_data, inverted_eta)

    
    
    power_result[i, ] <- LatVarfunc_result1[1:3]
    i<- i +1
    if(i > 100) break}
  return(sum(power_result[,1] > 0 | power_result[,3] < 0)/ 100)
}
sample_vector <- 200
##lat_var_power_results <- sapply(sample_vector, function(n_value) {lat_var_power_function(n_value)})
power_100_latvar<-0.83
power_200_latvar <- 0.97
```

Type I error rate estimation of LatVarfunc
*Takes a long time to run this, only ran it for sample sizes of 100 and 200*
Remove the double hashes to execute the type I error code
```{r}
set.seed(24253739)
type_1_error_func <- function(n) {
  power_result <- matrix(NA, nrow = 100, ncol = 3) 
  i <- 1
  repeat {

    sampled_data <- inverted_data[sample(nrow(inverted_data), n, replace = TRUE), ]
    sampled_data$binary_group <- sample(rep(c(0, 1), each = n / 2))

    inverted_eta <- c(-5.1, -5.1)

    LatVarfunc_result1<- LatVarfunc(sampled_data, inverted_eta)

    
    
    power_result[i, ] <- LatVarfunc_result1[1:3]
    i<- i +1
    if(i > 100) break}
  return(sum(power_result[,1] > 0 | power_result[,3] < 0)/ 100)
}
sample_vector <- 200
##type_1_error <- sapply(sample_vector, function(n_value) { type_1_error_func(n_value)})

LVF_type_1_error_rate_100 <- 0.49
LVF_type_1_error_rate_200 <- 0.57
```

P-value Estimation of LatVarFunc
```{r}
rd <- -0.043
LCI <- -0.0488
UCI <- -0.0372

se <- (rd-LCI)/1.96
to_get_zero <- rd/se
p_value_latvar <-2*(1-pnorm(abs(to_get_zero)))
```

## Power Estimation
Power estimation function
```{r}
power_function <- function(x, y, z, e, n) {
  summed_TS_power_result <- numeric(10000)  
  squared_TS_power_result <- numeric(10000)  
  EMERGE_composite_power <- numeric(10000)
  i <- 1
  repeat {
    placebo_group <- tmpDat[tmpDat$Group == "Placebo", ]
    placebo_group <- placebo_group[complete.cases(placebo_group[, c(x, y, z)]), ]
    sampled_placebo <- placebo_group[sample(nrow(placebo_group), n/2, replace = TRUE), ]
    
    metformin_group <- tmpDat[tmpDat$Group == "Metformin", ]
    metformin_group <- metformin_group[complete.cases(metformin_group[, c(x, y, z)]), ]
    sampled_metformin <- metformin_group[sample(nrow(metformin_group), n/2, replace = TRUE), ]

    
    t_test_result1<- t.test(sampled_metformin[x], sampled_placebo[x])
    t_test_result2<- t.test(sampled_metformin[y], sampled_placebo[y])
    t_test_result3<- t.test(sampled_metformin[z], sampled_placebo[z])
    t_test_result4<- t.test(sampled_metformin[e], sampled_placebo[e])
    
    
    summed_TS_power_result[i] <- (t_test_result1$statistic + t_test_result2$statistic + t_test_result3$statistic)
    squared_TS_power_result[i]<-(((t_test_result1$statistic)^2)+((t_test_result2$statistic)^2)+((t_test_result3$statistic)^2))
    EMERGE_composite_power[i] <- t_test_result4$p.value
    i<- i +1
    if(i > 10000) break}
  return(as.numeric(c(sum(summed_TS_power_result<quantile(permuted_null_distributions.df$permutation_summed_composite, 0.025)| summed_TS_power_result > quantile(permuted_null_distributions.df$permutation_summed_composite, 0.975))/ 10000,## permutation test summed
         sum(squared_TS_power_result>quantile(permuted_null_distributions.df$permutation_squared_composite, 0.95))/ 10000, ##permutation test squared
         sum(squared_TS_power_result > quantile(theoretical_null_squared$chi_sq, 0.95))/ 10000, ##asymptotic chi-square
         sum((EMERGE_composite_power< 0.05))/ 10000, ## EMERGE
         sum(summed_TS_power_result<quantile(null_normal_distribution, 0.025)| summed_TS_power_result > quantile(null_normal_distribution, 0.975))/ 10000)))
}
```

Applying power estimation function
```{r}
sample_vector <- seq(from = 30, to = 540, by = 10)
unified_power_results <- sapply(sample_vector, function(n_value) {
  power_function("gw32", "gw38", "binary_insulin", "binary_EMERGE", n_value)
})
unified_power_results_df <- as.data.frame(t(unified_power_results)) 
colnames(unified_power_results_df) <- c("Permuted: Summed TS", "Permuted: Squared TS", "Asymptotic Chi-square: Eigenvalue weights TS", "EMERGE Composite", "Asymptotic Normal: Equal weights TS")
unified_power_results_df$sample_size <- sample_vector

```

Plotting power estimation
```{r}
power_100_latvar<-0.83
power_200_latvar <- 0.97

LVF_power_df <- data.frame( power = c( 0.83, 0.97),
                                   sample_size = c(100, 200),
                                   label = "Latent Variable")
power_long <- unified_power_results_df %>%
  pivot_longer(
    cols = c("Permuted: Summed TS", "Permuted: Squared TS", "Asymptotic Chi-square: Eigenvalue weights TS", "EMERGE Composite", "Asymptotic Normal: Equal weights TS"),
    names_to = "composite_type",
    values_to = "power"
  )
write.csv(power_long, "power_long.csv", row.names = FALSE)

ggplot(power_long, aes(x = sample_size, y = power, color = composite_type)) +
  geom_line(size = 1, alpha = 0.5) +
  labs(
    title = expression(bold("")),
    x = "Sample Size",
    y = "Power"
  ) +
  scale_x_continuous(limits = c(30, NA), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 1.1)) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 16),
    axis.text = element_text(size = 12),
    plot.title = element_text(size = 24),
    legend.title = element_blank(),
    legend.text = element_text(size = 9, face = "bold"),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  ) +
  scale_color_manual(
    values = c(
      "Permuted: Summed TS" = "mediumseagreen",
      "Permuted: Squared TS" = "blue",
      "Asymptotic Chi-square: Eigenvalue weights TS" = "orange2",
      "EMERGE Composite" = "magenta",
      "Asymptotic Normal: Equal weights TS" = "firebrick"
    )
  ) + 
  guides(color = guide_legend(nrow = 3)) + 
  geom_point(data = LVF_power_df, aes(x = sample_size, y = power),
    shape = 21,
    fill = "black",
    color = "black",
    size = 2,
    stroke = 1.2,
    inherit.aes = FALSE,
    vjust = -1
  ) + 
  geom_text(data = LVF_power_df, aes(x = sample_size, y = power, label = label),
    vjust = -1,   
    size = 2,
    fontface = "bold",
    color = "darkred",
    inherit.aes = FALSE
  )
```

## Type I error rate Estimation
```{r}
set.seed(24253739)
type_I_error_function <- function(x, y, z, e, n) {
  summed_TS_power_result <- numeric(10000)  
  squared_TS_power_result <- numeric(10000)  
  EMERGE_composite_power <- numeric(10000)
  i <- 1
  repeat {
    sampled_data <- tmpDat[sample(nrow(tmpDat), n, replace = TRUE), ]
    sampled_data$binary_g <- rep(c(0, 1), each = n / 2) ##binary_g refers to treatment group
    sampled_metformin <- sampled_data[sampled_data$binary_g == 1,]
    sampled_placebo <- sampled_data[sampled_data$binary_g == 0,]

    
    t_test_result1<- t.test(sampled_metformin[x], sampled_placebo[x])
    t_test_result2<- t.test(sampled_metformin[y], sampled_placebo[y])
    t_test_result3<- t.test(sampled_metformin[z], sampled_placebo[z])
    t_test_result4<- t.test(sampled_metformin[e], sampled_placebo[e])
    
    
    summed_TS_power_result[i] <- (t_test_result1$statistic + t_test_result2$statistic + t_test_result3$statistic)
    squared_TS_power_result[i] <- (((t_test_result1$statistic)^2)+((t_test_result2$statistic)^2)+((t_test_result3$statistic)^2))
    EMERGE_composite_power[i] <- t_test_result4$p.value
    i<- i +1
    if(i > 10000) break}
  return(c(sum(summed_TS_power_result<quantile(permuted_null_distributions.df$permutation_summed_composite, 0.025) | summed_TS_power_result > quantile(permuted_null_distributions.df$permutation_summed_composite, 0.975))/ 10000, ##permuted summed composite
         sum(squared_TS_power_result>quantile(permuted_null_distributions.df$permutation_squared_composite, 0.95))/ 10000, ##permutetd squared composite
         sum(squared_TS_power_result > quantile(theoretical_null_squared$chi_sq, 0.95))/ 10000, ##theoretical squared composite
         sum((EMERGE_composite_power< 0.05))/ 10000, ##EMERGE composite
         sum(summed_TS_power_result<quantile(null_normal_distribution, 0.025)| summed_TS_power_result > quantile(null_normal_distribution, 0.975))/ 10000))
}
```

Applying power estimation code and plotting
```{r}
sample_vector <- seq(from = 30, to = 540, by = 10)
type_1_error_rate <- sapply(sample_vector, function(n_value) {
  type_I_error_function("gw32", "gw38", "binary_insulin", "binary_EMERGE", n_value)
})
type_1_error_rate_df <- as.data.frame(t(type_1_error_rate)) 
colnames(type_1_error_rate_df) <- c("Permuted: Summed TS", "Permuted: Squared TS", "Asymptotic Chi-square: Eigenvalue weights TS", "EMERGE Composite", "Asymptotic Normal: Equal weights TS")
type_1_error_rate_df$sample_size <- sample_vector


LVF_type_1_error_df <- data.frame( error_rate = c( 0.49, 0.57),
                                   sample_size = c(100, 200),
                                   label = "Latent Variable")

error_long <- type_1_error_rate_df %>%
  pivot_longer(
    cols = c("Permuted: Summed TS", "Permuted: Squared TS", "Asymptotic Chi-square: Eigenvalue weights TS", "EMERGE Composite", "Asymptotic Normal: Equal weights TS"),
    names_to = "composite_type",
    values_to = "error_rate"
  )
write.csv(error_long, "Type I error estimation.csv", row.names = FALSE)

ggplot(error_long, aes(x = sample_size, y = error_rate, color = composite_type)) +
  geom_line(size = 1, alpha = 0.5) +
  labs(
    title = expression(bold("")),
    x = "Sample Size",
    y = "Type 1 error"
  ) +
  scale_x_continuous(limits = c(30, NA), expand = c(0, 0)) +
  scale_y_continuous(limits = c(0, 0.1)) +
  theme_minimal() +
  theme(
    axis.title = element_text(size = 12),
    axis.text = element_text(size = 10),
    plot.title = element_text(size = 28),
    legend.title = element_blank(),
    legend.text = element_text(size = 11, face = "bold"),
    legend.position = "bottom",
    panel.grid.minor = element_blank()
  ) +
scale_color_manual(
    values = c(
      "Permuted: Summed TS" = "mediumseagreen",
      "Permuted: Squared TS" = "blue",
      "Asymptotic Chi-square: Eigenvalue weights TS" = "orange2",
      "EMERGE Composite" = "magenta",
      "Asymptotic Normal: Equal weights TS" = "firebrick"
    )
  ) + 
  guides(color = guide_legend(nrow = 3)) +
  geom_point(data = LVF_type_1_error_df, aes(x = sample_size, y = error_rate),
    shape = 21,
    fill = "black",
    color = "black",
    size = 2,
    stroke = 1.2,
    inherit.aes = FALSE,
    vjust = -1
  ) + 
  geom_text(data = LVF_type_1_error_df, aes(x = sample_size, y = error_rate, label = label),
    vjust = -1,   
    size = 3.5,
    fontface = "bold",
    color = "darkred",
    inherit.aes = FALSE
  )

```

## Simulating toy data set

pre-processing of tmpDat and acquiring parameters required for simulating data
```{r}
tmpDat <- tmpDat[(!is.na(tmpDat$InsulinYN) & !is.na(tmpDat$gw32) & !is.na(tmpDat$gw38)), ]
met_InsulinY_data <- filter(tmpDat, binary_insulin == 1, Group == "Metformin") 
pla_InsulinY_data <- filter(tmpDat, binary_insulin == 1, Group == "Placebo") 

met_InsulinN_data <- filter(tmpDat, binary_insulin == 0, Group == "Metformin")
pla_InsulinN_data <- filter(tmpDat, binary_insulin == 0, Group == "Placebo")


met_correlation_matrix_InsulinY <- cor(x=dplyr::select(met_InsulinY_data, gw32, gw38), use = "pairwise.complete.obs")
pla_correlation_matrix_InsulinY <- cor(x=dplyr::select(pla_InsulinY_data, gw32, gw38), use = "pairwise.complete.obs")

met_correlation_matrix_InsulinN <- cor(x=dplyr::select(met_InsulinN_data, gw32, gw38), use = "pairwise.complete.obs")
pla_correlation_matrix_InsulinN <- cor(x=dplyr::select(pla_InsulinN_data, gw32, gw38), use = "pairwise.complete.obs")

avg_Met_gw32_InsulinY<-mean(tmpDat$gw32[tmpDat$Group == "Metformin" & tmpDat$binary_insulin == 1], na.rm=T) ##Average gw32 for metformin group with Insulin
avg_Met_gw38_InsulinY<-mean(tmpDat$gw38[tmpDat$Group == "Metformin" & tmpDat$binary_insulin == 1], na.rm=T)## Same again for gw38

avg_Met_gw32_InsulinN<-mean(tmpDat$gw32[tmpDat$Group == "Metformin" & tmpDat$binary_insulin == 0], na.rm=T)##Average gw38 for metformin with no Insulin
avg_Met_gw38_InsulinN<-mean(tmpDat$gw38[tmpDat$Group == "Metformin" & tmpDat$binary_insulin == 0], na.rm=T)

##Now for Placebo
avg_Plcb_gw32_InsulinY<-mean(tmpDat$gw32[tmpDat$Group == "Placebo" & tmpDat$binary_insulin == 1], na.rm=T) 
avg_Plcb_gw38_InsulinY<-mean(tmpDat$gw38[tmpDat$Group == "Placebo" & tmpDat$binary_insulin == 1], na.rm=T)

avg_Plcb_gw32_InsulinN<-mean(tmpDat$gw32[tmpDat$Group == "Placebo" & tmpDat$binary_insulin == 0], na.rm=T)
avg_Plcb_gw38_InsulinN<-mean(tmpDat$gw38[tmpDat$Group == "Placebo" & tmpDat$binary_insulin == 0], na.rm=T)
```

Generating the toy data set (aka dummy data)
```{r}
fake_met_insulinY <- mvrnorm(nrow(met_InsulinY_data), c( avg_Met_gw32_InsulinY, avg_Met_gw38_InsulinY), met_correlation_matrix_InsulinY)
fake_met_insulinY <- data.frame(fake_met_insulinY)
fake_met_insulinY$Group <- "Metformin"

fake_pcb_insulinY <- mvrnorm(nrow(pla_InsulinY_data), c( avg_Plcb_gw32_InsulinY, avg_Plcb_gw38_InsulinY), pla_correlation_matrix_InsulinY)
fake_pcb_insulinY <- data.frame(fake_pcb_insulinY)
fake_pcb_insulinY$Group <- "Placebo"

fake_met_insulinN <- mvrnorm(nrow(met_InsulinN_data), c( avg_Met_gw32_InsulinN, avg_Met_gw38_InsulinN), met_correlation_matrix_InsulinN)
fake_met_insulinN <- data.frame(fake_met_insulinN)
fake_met_insulinN$Group <- "Metformin"

fake_pcb_insulinN <- mvrnorm(nrow(pla_InsulinN_data), c( avg_Plcb_gw32_InsulinN, avg_Plcb_gw38_InsulinN), pla_correlation_matrix_InsulinN)
fake_pcb_insulinN <- data.frame(fake_pcb_insulinN)
fake_pcb_insulinN$Group <- "Placebo"

fake_InsulinY_df <- data.frame(rbind(fake_met_insulinY, fake_pcb_insulinY))
fake_InsulinY_df$InsulinYN <- "Yes"
fake_InsulinY_df$ID<- c(1:(nrow(met_InsulinY_data) + nrow(pla_InsulinY_data)))
fake_InsulinY_df <- fake_InsulinY_df[, c("ID", "Group", "InsulinYN", "gw32", "gw38")]

fake_InsulinN_df <- data.frame(rbind(fake_met_insulinN, fake_pcb_insulinN))
fake_InsulinN_df$InsulinYN <- "No"
fake_InsulinN_df$ID<- c((nrow(met_InsulinY_data) + nrow(pla_InsulinY_data) + 1): nrow(tmpDat))
fake_InsulinN_df <- fake_InsulinN_df[, c("ID", "Group", "InsulinYN", "gw32", "gw38")]

dummy_data<- rbind(fake_InsulinY_df, fake_InsulinN_df)

dummy_data$binary_insulin<-0
dummy_data$binary_insulin[dummy_data$InsulinYN == "Yes"]<-1
                         
dummy_data$binary_group<-0
dummy_data$binary_group[dummy_data$Group == "Metformin"]<-1

write.csv(dummy_data, "Simulated toy dataset.csv", row.names = FALSE)
```

table of parameters for dummy data
```{r}
dummy_data_parameters <- data.frame(group = c("Metformin, InsulinY", "Metformin, InsulinN",
                                                 "Placebo, InsulinY", "Placebo, InsulinN"),
                                       n = c(nrow(met_InsulinY_data), nrow(met_InsulinN_data),
                                                  nrow(pla_InsulinY_data), nrow(pla_InsulinN_data)),
                                       mean_gw32 = c(avg_Met_gw32_InsulinY, avg_Met_gw32_InsulinN,
                                                     avg_Plcb_gw32_InsulinY, avg_Plcb_gw32_InsulinN),
                                       mean_gw38 = c(avg_Met_gw38_InsulinY, avg_Met_gw38_InsulinN,
                                                     avg_Plcb_gw38_InsulinY, avg_Plcb_gw38_InsulinN),
                                       correlation = c(met_correlation_matrix_InsulinY[1,2], met_correlation_matrix_InsulinN[1,2],
                                                       pla_correlation_matrix_InsulinY[1,2], pla_correlation_matrix_InsulinN[1,2]))
colnames(dummy_data_parameters) <- c("Group", "N", "Mean GW32", "Mean GW38", "Correlation")
dummy_data_parameters %>%
  kable(
    caption    = "Parameters of Simulated Toy Data",
    booktabs   = TRUE,
    digits     = c(0, 0, 2, 2, 2),
    align      = c("l", "r", "r", "r", "r")
  ) %>%
  kable_styling(
    latex_options = c("striped", "hold_position"), 
    full_width    = FALSE
  )
```

Toy data adapted for McMenamins app
Require baseline gw32 and gw38 values, which we set as 0 for all except two individuals, one of which has 0.1 as the baseline value for gw32 or gw38
Additionally, the values had to be "inverted" for the mcmenamin app to work properly. This was assessed by matching the risk difference calculated by the app to that reported in the EMERGE paper.
```{r}
simulated_data <- dummy_data %>% dplyr::select(patient_id = ID, treat = binary_group, Y1 = gw32, Y2 = gw38, Ybin = binary_insulin)
simulated_data$Y1_0 <- 0
simulated_data$Y2_0 <- 0
simulated_data$Y1_0[1] <- -0.1
simulated_data$Y2_0[2] <- -0.1
simulated_data$Ybin <- ifelse(simulated_data$Ybin == 1, 0, 1)
simulated_data$Y1 <- simulated_data$Y1 * -1
simulated_data$Y2 <- simulated_data$Y2 * -1
write.csv(simulated_data, "Simulated Inverted Emerge Data.csv", row.names = FALSE)

```



##Comparing Asymptotic and permutation null distributions
```{r}
asymptotic_null_summed_small <- read.csv("Theoretical Summed null distribution small.csv")
summed_composite_histogram.df <- data.frame(ts = c(permuted_null_distributions.df$permutation_summed_composite, asymptotic_null_summed_small$V1), group = rep(c("Permuted", "Theoretical"), c(length(permuted_null_distributions.df$permutation_summed_composite), nrow(asymptotic_null_summed_small))))
                                                          
ggplot(data = summed_composite_histogram.df, aes(x =ts, fill = group )) + geom_histogram(bins =250, alpha = 0.5, position = "identity") + labs(x = "Summed Composite Test Statistic (SC)", y = "Frequency") +
  theme_minimal() + theme(panel.grid.minor = element_blank()) +
  scale_fill_manual(values = c("yellow", "skyblue"), name = "Method")
```

```{r}
asymptotic_null_sq_small <- read.csv("Theoretical Squared null distribution small.csv")

squared_composite_histogram.df <- data.frame(ts = c(permuted_null_distributions.df$permutation_squared_composite, asymptotic_null_sq_small$V1), group = rep(c("Permuted", "Theoretical"), c(length(permuted_null_distributions.df$permutation_squared_composite), nrow(asymptotic_null_sq_small))))
                                                          
ggplot(data = squared_composite_histogram.df, aes(x =ts, fill = group )) + geom_histogram(bins =250, alpha = 0.5, position = "identity") + labs(x = "Squared Composite Test Statistic (QC)", y = "Frequency") +
  theme_minimal() + theme(panel.grid.minor = element_blank()) +
  scale_fill_manual(values = c("yellow", "skyblue"), name = "Method")
```

## References
 McMenamin, Martina, Jessica K Barrett, Anna Berglind, and James Ms Wason. 2021. “Employing a Latent
 Variable Framework to Improve Efficiency in Composite Endpoint Analysis.” Statistical Methods in
 Medical Research 30 (3): 702–16. https://doi.org/10.1177/0962280220970986.
